---
title: "DLO3"
excerpt: "Predicting summertime ozone using a hybrid deep learning model, with meteorological fields from the ERA5-Interim Analysis and NOx emission sections from CEDS. The figure below shows the mean observed and predicted MDA8 ozone during 2010-2014, with the error and correlation on the bottom half of the panel. The figure is from a colleagues' paper (who's work I continued) <br/>
<img src='/images/tailong_grl_result.png'>"
collection: portfolio
---

This project was undertaken Summer 2021. I was being guided by Professor Dylan Jones and senior PhD student Tai-Long He, both at the University of Toronto. I was working remotely with the 5 hour time difference (due to COVID). Fortunately, I was  able to attend the weekly group meetings on Thursdays 7pm with a cup of tea. I learnt about the group members' research, falling broadly under Atmospheric Modelling and Composition. As for my own research, a lot of the foundational work was completed by Tai-long He, which I've heavily drawn from (see figures below). The statistical analysis was guided by Professor Jones. It is with their help I've been able to complete this work. 

### Research

Following Tai-long [1], I further investigated the deep learning model used to predict summertime ozone concentrations over the US. We were interested in what gave the model it's temporal skill, and why it was able to capture the dynamics well. One way of approaching this problem was through explicitly looking at feature importance, by excluding certain meteorological features and then conducting a regional analysis. We hoped to gain some insight of what dynamics the model was learning. My analysis shows after feature exclusion, there were some differences in predictive skill and that these mainly dependent on region. The results were in agreement with Tai-long's findings, and emphasized the importance of including NOx emission sectors. There were many questions left after this analysis.

At the beginning of summer, I spent time reproducing most of the results in the archival paper (including appendices) by independently conducting my own model runs and analysis. With little previous experience, this gave me an opportunity to learn Deep Learning in Tensorflow and climate analysis with Xarray.  By  the end, I gave a semi-formal 40-minute presentation to the group. [You can find the slides here](https://github.com/apurba-biswas/apurba-biswas.github.io/blob/master/files/group_presentation.pdf)

An even more informal explanation lies below, which I hope is accessible, succinct and somewhat complete. Feel free to send me comments/questions/feedback on the work below.

## Why is ozone important?

Tropospheric (ground-level/near-surface) ozone, is a major air pollutant and a greenhouse gas. As an air pollutant, ozone risks those who are already have a compromised respiratory systems (e.g. asthma) and children. It's oxidative capability also impacts crop yield. As a greenhouse gas, it is much more **potent than CO2**.


### Why is this a tricky problem?

Ozone is not directly emitted by most anthropogenic activity. Instead, it's produced in the atmosphere by a chemical reaction, composed of NOx (nitrous oxides), VOCs (volatile organic compounds) and UV radiation. Due to the photochemical reaction, ozone concentrations are high during summertime. This reaction, however is complicated and non-linear. 

A recent example demonstrating this complexity would be the concentration during the COVID-19 pandemic. Upon lockdown in several countries (China, India, US, UK), NOx emissions from transport (and indirectly other sectors) were reduced. However, this would not immediately be obvious in the ozone concentrations.

_Atmospheric models used to simulate the distribution of ozone typically do not reproduce the observed long-term trend in tropospheric ozone. Furthermore, these models tend to overestimate summertime surface ozone abundances in the United States._ [1]

## The model, data and previous results

Being inspired by Shi et al's precipitation nowcasting [3], Tai-long devised a Recurrent U-Net model, to capture spatial and temporal variation in ozone concentrations over the US. The hybrid model consists of stacked CNNs and LSTMs, to learn spatial correlations and dynamics respectively. Use of skip-connections preserves higher-level features of the model.

![](../images/model_schematic.png)

He utilized 6 daily meteorological fields (drivers of ozone chemistry and tropospheric conditions) from the ERA-5 Interim Reanalysis, and monthly mean NOx emissions from CEDS (community emissions data system). These were gridded at a spatial resolution of $1.5^o \times 1.5^o$.

For initial model development and my study, we worked with the 1980-2014 dataset, with the first 30 summers used for training, and the rest for testing and model evaluation. Tai-long used to model to evaluate other inventories. 

The figures below are from paper submitted to Geophysical Research Letters [2]

![](../images/tailong_grl_result.png)

Figure 2. Observed (top left) and predicted (top right) mean JJA MDA8 ozone during 2010– 2014. Also shown (bottom left) is the absolute error (in ppb) for the predicted minus observed MDA8 ozone. The errors are calculated where the AQS observations are located. Correlation (r2) between the observed and predicted MDA8 ozone in each grid box is shown in bottom right. Also shown in bottom right are the definitions of the CONUS, Northeastern US, Southeastern US and the West coast domains in blacked dashed box, black solid box, blue box, and green box, respectively.

![](../images/conus_time_series.png)

Figure 3. Observed (blue line) and predicted (orange line) daily (first column), 7-day averaged (second column), and monthly averaged (third column) JJA MDA8 ozone (in ppb) during the testing period (2010–2014). Shown are the time series for the CONUS (first row) ...

(see paper for full figure)

The model is able to capture both short and long-term variability.
_The model is able to capture 85%, 86%, and 76% of the variability of MDA8 in the northeastern, southeastern, and western United States, respectively_ [1]

_The large negative bias on the West Coast (see Table 1) and in the central United States (Fig. 2) could be due to the absence of emissions of NOx from soils in the predictors, which have been shown to be a major source of NOx in these regions_

I find this regional skill interesting as there is more variability MDA8 ozone in the East than the west as by the AQS observation. I've plotted the mean and standard deviation of the MDA8 ozone (ppb) below.

![](../images/mean_std_obs.png)

The question of "What the model is learning?" is a tricky one, prevalent in deep learning. We've tackled it in this specific context. This is partly addressed by looking at the activation maps learned by the convolutional layers of the model's encoder [1], though apart from telling us overall regional importance, it's hard to further analyse. I layout potential solutions to this problem later under "Future work".


## Meteorological feature exclusion and regional analysis

What we initially wanted to explore is what gave the model it's temporal skill. We were hoping to discover any insights of potential physical phenomena learned by the model (on potentially subseasonal timescales). The deep-learning answer would be to interpret the stacked LSTM cells. However, at the time of searching, I wasn't able to find an accessible way to do this. The complexity of the model made this an even more daunting task. Therefore, we decided on a more simpler approach, where we would remove a meteorological predictor at a time, and look at the model performance, in different regions. 

One of the experiments ran by Tai-long was to train the model with just the meteorological features. The model was able to capture the variability with similar correlation scores as above. However, the model was positively biased overestimating ozone concentrations. This demonstrated the importance of including NOx emissions.

Hence, to study what features were important to capture the variability, it would be sufficient to just use the met. fields. However, when conducting a regional analysis and studying potential biases, it would be tricky to diagnose model performance. Therefore, when continuing ahead, I decided to use all but one predictor, excluding just one of the meteorological field at a time.

After training the models in these configurations, we test their predictive skill over 2010-2014. For reference, the 6 meteorological features excluded and their respective model names were
1. geopotential at 500 hPz (Z) -> no_geop
2. 2 meter dew point -> no_d2m
3. mean sea level pressure -> no_mslp
4. 2 meter temperature -> no_t2m
5. downward shortwave radiation -> no_ssrd
6. sea surface temperature -> no_sst

Collectively, I refer to these as featureless models.

## Results

![](../images/featureless_plots.png)

The figure above shows the performance of the model over the 2010-2014 test period.

1) The left panel shows the predicted ozone time-series for the different models. Observation (truth) in blue and predicted in orange. The pearsons correlation co-effecient r is calculated (**mislabled as r2**)
2) The right panel is a scatter of the mean observation against the mean predicted ozone. Dashed line is the one-to-one line. The fit of the "all_features" model is in blue, with the fit of the featureless model is in red.

Focusing on the time-series plot, it is clear that removal of a feature degrades predictive skill, with some variance across the models (r2 from 0.6 ~ 0.66). Feature importance is not immediately clear, with some featureless models with a slight positive/negative bias.

The scatter on the right shows that removal of a meteorological feature also biases the model, with a negative bias in the high-ozone regime and vice versa. However, as we're working with MDA8, there's not much low ozone to study. With the scatter plot, we were trying to determine how each model would perform in low/high ozone regime. Clusters in the scatter would sign towards anomalous behavior. Though, all models perform similarly and feature importance is not clear with little variance (r2 from 0.81 ~ 0.87).

### Regional Analysis and feature importance

Another way of looking at the dynamical impacts of removing a feature was to conduct a regional analysis, as there would be different systems at play in different region. This more direct sensitivity analysis is parallel to the looking at the activation maps from the convolutional layers of the model, another way of looking at regional/local feature importance.

The baseline model is the all features model, which we compare against the featureless (without the met. field) models. We're trying to see where these models differ.

**Difference in all_features and featureless**

The difference of the two models (all_features - featureless) is plotted below. The mean and median difference are calculated.

![](../images/absolute_diff.png)

From these, we can tell that overall over CONUS, the models do not vary significantly, but with some grids being poorly predicted. The only models that differ by more than 1ppb are the featureless models without geopotential (geop) and sea surface temperature (sst).

One of the few features that is consistent across all the models is the positive bias in the intermountain west. One factor is the lack of observations in this region, and hence further reduced skill. Furthermore, background ozone concentrations are influenced by ozone produced by lightning in the upper atmosphere, and carried down by stratospheric intrusions.[4]

On the Gulf Coast, all models except the featureless geopotential model, show a negative bias, with the d2m model showing significant bias. It is possible the seabreeze effect is being captured by the all_features model, and removal of a relevant predictor reduces the degree to where the phenomena is captured. 

**The difference in correlation**

Here, a pair-wise difference in correlation to the observation.

1. correlation of all_features model and the observation
2. correlation of the featureless model and observation

Then, we've plotted 1-2. Hence, negative blue values mean that the featureless model performs better than the all features model.

![](../images/correlation_difference.png)

The result is surprising, as I would've expected the opposite. The plots may seem saturated with blue, especially in the intermountain west. I believe again this is due to the lack on information in that region. However, the magnitude of these differences are insignificant. Given the better temporal skill of the all_features model over CONUS, this grid view hints to the generalization of the model.

The regions are red are also of interest, as they are where the all_features model perform better, hence, removing a feature decreases predictive skill in these regions.  The featureless d2m model and temperature model show a large region in the East/North East respectively where this is the case.

However, due to the magnitude of this effect, feature importance is once again not clear.

## Zooming into different regions

Similar to Tai-long's analysis, we zoomed into different regions over CONUS, and looked at the predicted time-series and also the observation vs prediction scatter, similar to the initial analysis.

5 different regions were analyzed, with  some overlap. This procedure is flexible, and the lat-lon bounding boxes can be easily defined. The regions were similar to the analysis in Tailong's paper. 3/5 regions are presented below (please look at the slides for North East and the West Coast). The number of grid boxes with observations are also displayed.

When writing about the analysis below, I've used
1) to talk about the time series plot
2) to talk about the scatter plot

to avoid repetition.


For the scatter plot, I've included
- the one-to-one observation line in dashed black
- the blue dots are the individual predictions
- the red line is the fit of the predicted ozone (linear regression with m being the gradient, r being pearsons correlation co-efficient)

&nbsp;

**Note: on the time-series, I've mislabeled the r score as r2**


![](../images/south_east.png)

1. There's some variability in the predictive skill of the models, with featureless d2m having the worst skill, and featureless sst in next place. I think this points to the seabreeze effect as mentioned earlier.
2. Overall, mean ozone is captured well amongst the models with little variability. There's a clear positive bias in the featureless d2m model. Featureless sst has the weakest fit.

![](../images/mountain_west.png)

We chose to study this region due to the lack of observations in the data.

1. Weak temporal predictive skill
2. Scatter in the models

![](../images/north.png)

1. Though part of this region overlaps with the intermountain west (only a few locations in the upper left corner of the bounding box), we still have good temporal skill in this region, with little spread across the models. 

2. This region was interesting because there are noticeable differences in the scatter plot, unlike the other regions studied. For example, the featureless d2m model captures mean ozone well, as opposed to the featureless ssrd model. This region hints more strongly at feature importance than the other regions. 

Predictability in this region is difficult.

### How to interpret this?

From Tailong's analysis, we already knew that the skill of the all_features model was region dependent. I think my analysis above hints to the degree its regional. 

## Future work

SHAP



## References
   
1. see Tailong's repo DLO3 (forked on my GitHub)
2. Tai-Long He, Dylan B. A. Jones, Binxuan Huang, Yuyang Liu, Kazuyuki Miyazaki, Zhe Jiang, E. Charlie White, Helen M. Worden, John R. Worden : Deep learning to evaluate US NOx emissions using surface ozone predictions (under review)
3. X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-k. Wong, and W.-c. WOO. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems 28. 2015.
4. Zhang, L., Jacob, D. J., Yue, X., Downey, N. V., Wood, D. A., and Blewitt, D.: Sources contributing to background surface ozone in the US Intermountain West, Atmos. Chem. Phys., 14, 5295–5309, https://doi.org/10.5194/acp-14-5295-2014, 2014.
