---
title: "DLO3"
excerpt: "Predicting summertime ozone using a hybrid deep learning model, with meteorological fields from the ERA5-Interim Analysis and NOx emission sections from CEDS. The figure below shows the mean observed and predicted MDA8 ozone during 2010-2014, with the error and correlation on the bottom half of the panel. The figure is from a colleagues' paper (who's work I continued) <br/>
<img src='/images/tailong_grl_result.png'>"
collection: portfolio
---

This project was undertaken Summer 2021. I was being guided by Professor Dylan Jones and senior PhD student Tai-Long He, both at the University of Toronto. I was working remotely with the 5 hour time difference (due to COVID). Fortunately, I was  able to attend the weekly group meetings on Thursdays 7pm with a cup of tea. I learnt about the group members' research, falling broadly under Atmospheric Modelling and Composition. As for my own research, a lot of the foundational work was completed by Tai-long He, which I've heavily drawn from (see figures below). The statistical analysis was guided by Professor Jones. It is with their help I've been able to complete this work. 

### Research

Following Tai-long [1], I further investigated the deep learning model used to predict summertime ozone concentrations over the US. We were interested in what gave the model it's temporal skill, and why it was able to capture the dynamics well. One way of approaching this problem was through explicitly looking at feature importance, by excluding certain meteorological features and then conducting a regional analysis. We hoped to gain some insight of what dynamics the model was learning. My analysis shows after feature exclusion, there were some differences in predictive skill and that these mainly dependent on region. The results were in agreement with Tai-long's findings, and emphasized the importance of including NOx emission sectors. There were many questions left after this analysis.

At the beginning of summer, I spent time reproducing most of the results in the archival paper (including appendices) by independently conducting my own model runs and analysis. With little previous experience, this gave me an opportunity to learn Deep Learning in Tensorflow and climate analysis with Xarray.  By  the end, I gave a semi-formal 40-minute presentation to the group. [You can find the slides here](https://github.com/apurba-biswas/apurba-biswas.github.io/blob/master/files/group_presentation.pdf)

An even more informal explanation lies below, which I hope is more accessible, succinct and somewhat complete. Feel free to send me comments/questions/feedback on the work below.

## Why is ozone important?

Tropospheric (ground-level/near-surface) ozone, is a major air pollutant and a greenhouse gas. As an air pollutant, ozone risks those who are already have a compromised respiratory systems (e.g. asthma)and children. It's oxidative capability also impacts crop yield. As a greenhouse gas, it is much more **potent than CO2**.


### Why is this a tricky problem?

Ozone is not directly emitted by most anthropogenic activity. Instead, it's produced in the atmosphere by a chemical reaction, composed of NOx (nitrous oxides), VOCs (volatile organic compounds) and UV radiation. Due to the photochemical reaction, ozone concentrations are high during summertime. This reaction, however is complicated and non-linear. 

A recent example demonstrating this complexity would be the concentration during the COVID-19 pandemic. Upon lockdown in several countries (China, India, US, UK), NOx emissions from transport (and indirectly other sectors) were reduced. However, this would not immediately be obvious in the ozone concentrations.

_Atmospheric models used to simulate the distribution of ozone typically do not reproduce the observed long-term trend in tropospheric ozone. Furthermore, these models tend to overestimate summertime surface ozone abundances in the United States._ [1]

## The model, data and previous results

Being inspired by Shi et al's precipitation nowcasting [3], Tai-long devised a Recurrent U-Net model, to capture spatial and temporal variation in ozone concentrations over the US. The hybrid model consists of stacked CNNs and LSTMs, to learn spatial correlations and dynamics respectively. Use of skip-connections preserves higher-level features of the model.

![](../images/model_schematic.png)

He utilized 6 daily meteorological fields (drivers of ozone chemistry and tropospheric conditions) from the ERA-5 Interim Reanalysis, and monthly mean NOx emissions from CEDS (community emissions data system ). These were gridded at a spatial resolution of $1.5^o \times 1.5^o$.

For initial model development and my study, we worked with the 1980-2014 dataset, with the first 30 summers used for training, and the rest for testing and model evaluation.

The figures below are from paper submitted to Geophysical Research Letters [2]

![](../images/tailong_grl_result.png)

Figure 2. Observed (top left) and predicted (top right) mean JJA MDA8 ozone during 2010– 2014. Also shown (bottom left) is the absolute error (in ppb) for the predicted minus observed MDA8 ozone. The errors are calculated where the AQS observations are located. Correlation (r2) between the observed and predicted MDA8 ozone in each grid box is shown in bottom right. Also shown in bottom right are the definitions of the CONUS, Northeastern US, Southeastern US and the West coast domains in blacked dashed box, black solid box, blue box, and green box, respectively.

![](../images/conus_time_series.png)

Figure 3. Observed (blue line) and predicted (orange line) daily (first column), 7-day averaged (second column), and monthly averaged (third column) JJA MDA8 ozone (in ppb) during the testing period (2010–2014). Shown are the time series for the CONUS (first row) ...

(see paper for full figure)

The model is able to capture bot short and long-term variability.

_The model is able to capture 85%, 86%, and 76% of the variability of MDA8 in the northeastern, southeastern, and western United States, respectively_ [1]

_The large negative bias on the West Coast (see Table 1) and in the central United States (Fig. 2) could be due to the absence of emissions of NOx from soils in the predictors, which have been shown to be a major source of NOx in these regions_

I find this regional skill interesting as there is more variability MDA8 ozone in the East than the west as by the AQS observation. I've plotted the mean and standard deviation of the MDA8 ozone (ppb) below.

![](../images/mean_std_obs.png)

The question of "What the model is learning?" is a tricky one, prevalent in deep learning tricky. We've tackled it in this specific context. This is partly addressed by looking at the activation maps learned by the convolutional layers of the model's encoder [1], though apart from telling us overall regional importance, it's hard to further analyse. I layout potential solutions to this problem later under "Future work".


## Meteorological feature exclusion and regional analysis

What we initially wanted to explore is what gave the model it's temporal skill. We were hoping to discover any insights of potential physical phenomena learned by the model (on potentially subseasonal timescales).The deep-learning answer would be to interpret the stacked LSTM cells. However, at the time of searching, I wasn't able to find an accessible way to do this. The complexity of the model made this an even more daunting task. Therefore, we decided on a more simpler approach, where we would remove a meteorological predictor at a time, and look at the model performance, in different regions. 

One of the experiments ran by Tai-long was to train the model with just the meteorological features. The model was able to capture the variability with similar correlation scores as above. However, the model was positively biased overestimating ozone concentrations. This demonstrated the importance of including NOx emissions.

Hence, to study what features were important to capture the variability, it would be sufficient to just use the met. fields. However, when conducting a regional analysis and studying potential biases, it would be tricky to diagnose model performance. Therefore, when continuing ahead, I decided to use all but one predictor, excluding just one of the meteorological field at a time.

After training the models in these configurations, we test their predictive skill over 2010-2014. For reference, the 6 meteorological features excluded and their respective model names were
1. geopotential at 500 hPz (Z) -> no_geop
2. 2 meter dew point -> no_d2m
3. mean sea level pressure -> no_mslp
4. 2 meter temperature -> no_t2m
5. downward shortwave radiation -> no_ssrd
6. sea surface temperature -> no_sst

Collectively, I refer to these as featureless models.

![](../images/featureless_plots.png)

The figure above shows the performance of the model over the 2010-2014 test period.

1) The left panel shows the predicted ozone time-series for the different models. Observation (truth) in blue and predicted in orange. The pearsons correlation co-effecient r is calculated.
2) The right panel is a scatter of the mean observation against the mean predicted ozone. Dashed line is the one-to-one line. The fit of the "all_features" model is in blue, with the fit of the featureless model is in red.

Focusing on the time-series plot, it is clear that removal of a feature degrades predictive skill, with some variance across the models (r2 from 0.6 ~ 0.66). Feature importance is not immediately clear, with some featureless models with a slight positive/negative bias.

The scatter on the right shows that removal of a meteorological feature also biases the model, with a negative bias in the high-ozone regime and vice versa. However, as we're working with MDA8, there's not much  This is in slight contrast with Tai-long's experiment of excluding NOx emissions. With the scatter plot, we were trying to determine how each model would perform in low/high ozone regime. Clusters in the scatter would sign towards anomalous behavior. Though, all models perform similarly and feature importance is not clear with little variance (r2 from 0.81 ~ 0.87).

### Regional Analysis

Another way of looking at the dynamical impacts of removing a feature was to conduct a regional analysis, as there would be different systems at play in different region. This more direct sensitivity is parallel to the looking at the activation maps from the convolutional layers of the model.

The baseline model is the all features model, which we compare against the featureless (without the met. field) models. We're trying to see where these models differ.

The mean error is plotted below (across time) is plotted below

![](../images/absolute_diff.png)

**The difference in correlation plot**


#### Zooming into different regions

![](../images/correlation_difference.png)

### Future work

SHAP



## References
   
1. see Tailong's repo DLO3 (forked on my GitHub)
2. Tai-Long He, Dylan B. A. Jones, Binxuan Huang, Yuyang Liu, Kazuyuki Miyazaki, Zhe Jiang, E. Charlie White, Helen M. Worden, John R. Worden : Deep learning to evaluate US NOx emissions using surface ozone predictions (under review)
3. X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-k. Wong, and W.-c. WOO. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems 28. 2015.
